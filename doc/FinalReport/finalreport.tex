\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage{color}
\usepackage{braket}
\usepackage{pgfgantt}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{pdfpages}
\usepackage{verbatim}
\usepackage[table,xcdraw]{xcolor}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\graphicspath{{Figures/}}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=25mm,
 right = 25mm,
 top=30mm,
 bottom=35mm
 }

\usepackage{fancyhdr}
\usepackage{lipsum}% just to generate text for the example

\pagestyle{fancy}
\fancyhf{}
\fancyhead[ER]{\nouppercase\leftmark}
\fancyhead[OR]{\nouppercase\rightmark}
\fancyhead[ER,OL]{\thepage}

\newcommand{\detailtexcount}[1]{%
  \immediate\write18{texcount -merge #1.tex > #1.wcdetail }%
  \verbatiminput{#1.wcdetail}%
}

\newcommand{\newp}
    {
    \vskip 0.5cm 
  }

\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}

\usepackage[backend=biber]{biblatex}

\addbibresource{diss.bib}

\numberwithin{equation}{section}

\nocite{*}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


%TC:ignore
\title{Analysis of the Quantum Advantages for Deep Hedging}

%TC:endignore

\author{Soham Deshpande\\Supervisor: Dr. Srinandan Dasmahapatra
\\Second Supervisor: Dr. Hansung Kim}
\date{April 2025}

\begin{document}

\maketitle
\vspace{10cm}
\begin{centering}
A project report submitted for the award of MEng Computer Science
\end{centering}
\thispagestyle{empty}

%TC:ignore
%\detailtexcount{progress}
%TC:endignore






\clearpage

\pagenumbering{roman}

%TC:ignore
\begin{abstract}
Parameterised Quantum Circuits (PQCs) have opened many doors, one 
such being the use in financial markets. In this paper, I look at the problem 
of developing an accurate market generator through the use of quantum computing
for the purposes of hedging. 
Given a Quantum Circuit Born Machine (QCBM), we are able to exploit the high 
expressibility to generate synthetic 
data that mimics the statistical distribution of the original dataset. The market 
generator is then used to simulate an 
underlying asset to maturity with the intent of learning an optimal hedging strategy 
$\pi^*$, a showcase of a data-driven approach to hedging exposure. I show that 
the synthetic 
data produced by this method has shown to capture the fat tails of the market 
better than classical methods, as well as demonstrating superiority in 
out-of-sample testing with COVID data. Different generator 
architectures have been compared to maximise the quality of the synthetic data 
and avoid issues with barren plateaus. 
The findings of 
this research will contribute to the growing literature on risk management in 
quantitative finance, with applications of the market generator extending beyond 
deep hedging. 
\end{abstract}

\clearpage


\tableofcontents
%TC:endignore
\clearpage



\pagenumbering{arabic}


\section{Problem Statement}
The problem of hedging a portfolio of derivatives is an important part of 
risk management used widely in financial institutions. This involves understanding 
the exposure to the market, and taking strategic positions to negate some of this 
risk. In an ideal world we can picture a perfect, 
frictionless market where transaction costs are negligible and every asset in 
the space has a price; here we can price and hedge perfectly. Unfortunately in 
practice, we experience incomplete markets due to frictions, costs that interfere 
with trades such as transaction costs or imperfect information. In addition, recent 
years have presented markets with periods of heightened volatility, much that disobey 
traditional frameworks. This generates the need for complex, realistic market 
models that can account for these.
\newp
Traditional methods of hedging options has shown to be ineffective for equity 
markets, new information resulting in rapid changes. Much of the available 
literature models the market as a smooth, continuous stochastic process within 
a Gaussian space. Such models are sufficient for common market activity but fail 
when presented with discontinuous moves in price. These can be reactions to 
geopolitical events or natural disasters; traditional
models are incapable of capturing the effects. The introduction of Jump-diffusion 
models aimed to solve this issue though face similar issues. In reaction, we have 
recently observed non-parametric models that harness neural networks and machine 
learning which aim to demonstrate high accuracy on out-of-sample forecasts.
\newp
An alternative approach that has recently emerged utilises the power of 
quantum computing. The introduction of parameterised quantum circuits(PQCs) have 
opened up new pathways for navigating complex, large scale time series datasets. 
Through rotation gates and quantum entanglement, we are able to learn complex 
distributions and relationships. 
\newp
In this research, I aim to tackle the problem 
of generating synthetic financial data, addressing issues that come about from 
using a classical method particularly the estimation of tail risk and skewness. 
Through comparisons between traditional approaches, 
I aim to demonstrate an advantage in the expressibility of Quantum Circuit Born 
Machines (QCBMs); these will be described quantitatively using measures such as 
Value at Risk (VaR) and Conditional Value at Risk (CVaR). By performing out-of-sample 
tests on COVID and Oil stock price data, I aim to highlight the weaknesses of traditional models 
and showcase a quantum superiority.
There will also be an exploration into the variety of architectures 
available for the QCBM, evaluating different ansatz designs. Where unusual behaviours
due to the quantum nature occur, such as barren plateau, I will explore in greater detail 
as well as any circuit optimisation techniques that may present themselves as 
possible solutions. 
\newp
This paper will aim to add to the existing literature on risk management for 
financial firms as well as providing a framework for generating synthetic 
data. In addition to that, I aim to extend to the QCBM research that exists 
currently, noting down any behaviours that may be of interest to the curious 
and potential 
experts in the field. 
\clearpage
\subsection{Aims \& Goals}
This section aims to provide a high-level insight into the goals of the 
project with more detailed explanations being left for their respective sections. 
\newp 
Market data, though appearing entirely stochastic at first glance, contains many 
hidden patterns and relationships. Classical methods such as Black-Scholes  
\autocite{blackscholes} have been proposed to capture some of this behaviour, 
though built on unrealistic assumptions, these govern 
our fundamental understanding of quantitative finance. With the move to the new era 
of electronic trading, the need for more complex markets models became more present.
Being able to trade within nanoseconds presented traders with a new, human-free method 
of trading, moving to a purely mathematical approach. In this approach we strive
for optimal strategies to make money, taking calculated positions and limiting 
exposure to large market movements. It is here where traditional methods started 
to be insufficient; particua
\clearpage 


\section{Related Literature}
To place this research within the context of existing literature, we can split 
the project into 2 components: the market generator, and 
parameterised quantum circuits.
\newp
The work around deep hedging has evolved, moving away from 
Greek-based hedging towards a sturdier framework using machine 
learning. Here a lot of work is being done, with many papers emphasising 
on using neural networks for optimising delta and gamma exposure
\autocite{armstrong_deep_2024,qiao_enhancing_2024}.
Buehler introduced an approach, modelling trading decisions 
as neural networks instead of relying on parameterised models
\autocite{buehler_deep_2019}. Subsequent advancements focussed on developing 
realistic market simulators. Wissel 
proposed a market model for path generation of options but this still 
employed risk-neutral diffusion\autocite{schweizer_arbitrage-free_2008}. Wiese 
then introduced 
a new dimension by using GANs to convert options into 
local volatility models with simpler no-arbitrage constraints. This focussed 
on the local stochastic nature of options
\autocite{choudhary_funvol_2023,wiese_deep_2019,wiese_multi-asset_2021}.
Some approaches suggest using actor-critic reinforcement learning algorithms to 
solve for an optimal value function, a move towards searching for a global
maximum over local risk management
\autocite{buehler_deep_2022,movahed_introducing_2024}.
\newp
Recent research explores using quantum computing to 
hedge portfolios, here the authors presented a quantum reinforcement learning 
method based on policy-search and distributional actor-critic algorithms. 
They proposed using a Quantum Neural Network to approximate the value of a 
given value function by predicting the expected utility of returns using compound 
and orthogonal layers which were built using Hamming-weight
unitaries \autocite{kerenidis_classical_2022}. 
\newp TO CHANGE:
This helped overcome the barren 
plateau by ensuring the gradient 
variance does not vanish exponentially with qubit count. 
\newp
Another method models 
the entire return distribution, leveraging parameterised circuits to learn categorical 
distributions and capture variability and tail risk 
\autocite{cherrat_quantum_2023,dasgupta_loading_2022}.
\newp
There is an immense amount of research being done on exploiting the benefits of 
quantum computing, recent advancements being in quantum algorithms. 
These claim to provide exponential speed-up over classical 
methods, though in reality, we see great complexity in state preparation, requiring 
$\Theta(2^n/n)$ circuit depth with n qubits or $\Theta(n)$ circuit depth with 
$\Theta(2^n)$ ancillary qubits\autocite{zhang_quantum_2022}. Here we see hybrid 
models such as Born machines
and Quantum generative adversarial networks boasting high generalisation ability
\autocite{ganguly_implementing_nodate,gili_2022_do,horowitz_quantum_2022}.
\newp
There has also been research in harnessing back action from quantum weak 
measurements to enhance the ability of quantum machine learning algorithms. 
In quantum reservoir computing,
the ability to retain information from past inputs plays a key role in processing 
temporal series and producing future predictions
\autocite{franceschetto_harnessing_2024,fujii_quantum_2020,garcia-beni_squeezing_2024,mujal_time-series_2023}.
\newp
This research aims to combine the needs of financial firms in hedging portfolios
using realistic market models by utilising QCBMs as a tool 
for simulating paths in combination with deep hedging engines for learning 
optimal policies. A comparison will be made against hedging under Merton-Jump 
diffusion. 
\clearpage
\section{Markets and Derivatives}
The market, though inherently can be thought of as a completely random process,
where bids and asks are fulfilled, can be modelled as a stochastic process. The 
aim of this chapter is to serve as a brief introduction and set up notation for 
later chapters. \\

\subsection{Brownian Motion}
To represent this stochasticity, we must employ techniques introduced by Norbert 
Wiener, the Wiener process, more commonly referred to as standard Brownian Motion. 
This framework allows us to model continuous random 
walks of our stock price. Formally, a standard Wiener process, $W_t$, is a stochastic 
process where 
\begin{enumerate}
  \item $W_0$ = 0
  \item The process $W_t$ has stationary, independent increments
  \item $\forall t \in Z,$ the random variable $W_t$ is normally distributed, $N(0,t)$ 
  \item The paths of $W_t$ are continuous ensuring no jumps in the path trajectory
\end{enumerate}
These assumptions will help us understand the shortfalls of traditional techniques.
\subsubsection{It\^{o} Process}
It\^{o} processes are crucial for understanding the mathematical set up for 
modelling our assets. It\^{o} calculus allows us to extend our understanding of 
deterministic calculus to the realm of stochasticity. 
Suppose $X_t$  It\^{o} process can be 
defined as a stochastic process which can be written in the form 
\begin{equation}
  X_t = X_0 + \int^t_0 U_s ds + \int^t_0 V_s dW_s
\end{equation}

\subsubsection{Geometric Brownian Motion}
We can extend Brownian Motion to Geometric Brownian Motion by exponentiating the BM; 
this is done to satisfy the condition that stock prices are non-negative. 
We can now consider a continuous time process $S(t)$ which satisfies the SDE
\begin{equation}
  dS_t = \mu S(t)dt + \sigma S(t)dW_t 
\end{equation}
where $\mu$ is the drift parameter, $\sigma$ is the volatility parameter, and 
$W_t$ is a Wiener process. The solution is given by 
\begin{equation}
  S_t = S_0 \exp \left[(\mu-\frac{1}{2}\sigma^2) t + \sigma W_t\right]
\end{equation}

\subsection{Market}
Consider a market with a finite time horizon $T$ defined on the probability space 
($\Omega,\mathcal{F},P$) along with a filtration $\bold{F} = \{\mathcal{F}| 0 \leq t \leq T \}$ 
This can be thought of as an adapted (n+1) dimensional It\^{o} process 
$X(t) = (X_0(t), X_1(t),...,X_n(t))$
which has the form 
\begin{equation}
dX_0(t) = \rho(t,\omega)X_0(t)dt;\hspace{8pt}X_0(0)=1
\end{equation}and
\begin{equation}
dX_i = \mu_i(t,\omega)dt+\sigma_i(t,\omega)dB(t);\hspace{8pt}X_i(0)=x_i 
\end{equation}
where $X_i(t)$ as the price of asset $i$ at a given time $t$.
\\
We can define a portfolio in the market as 
\begin{equation}
\theta(t,\omega) = (\theta_0(t,\omega),\theta_1(t,\omega),...,\theta_n(t,\omega))
\end{equation}
where the components $\theta_n(t,\omega)$ represents the number of units of a given 
asset held at time $t$.\\
Following from that, we can define the value of a given portfolio to be 
\begin{equation}
  V(t,\omega) = V^{\theta}(t,\omega) = \sum_{i=0}^n \theta_i(t)X_i(t)
\end{equation}
Lastly, it is important to state that the portfolio is self-financing, any 
trading strategy $\alpha$ requires no extra cost beyond the initial capital
\begin{equation}
  V(t) = V(0) + \int_0^t\theta(s)\cdot dX(t)
\end{equation}
We can also make the following assumptions about the market:
\begin{itemize}
  \item The market is liquid, allowing the trade to execute instantaneously
  \item There is no bid-ask spread, the price to buy and sell is the same 
  \item Trading actions taken have no impact on the price of the asset traded 
\end{itemize}


\subsection{Derivatives}
A derivative refers to any financial instrument whose value is derived from an 
underlying security, the most fundamental being futures and options. It is common 
practice to refer to the given underlying security as just 'underlying'.
\subsubsection{Futures}
A futures contract is a contract that gives the right and obligation to buy a 
given asset $i$ a specified time $T$ at price $K$. 
\subsubsection{Options}
The two types of options going to be explored are Puts and Calls; a Call option 
gives the owner the right but not the obligation to buy a given asset $i$ at 
a specified price $K$ at time $T$. Similar to the Call, a Put option gives the 
owner the right but not the obligation to sell a given asset $i$ at a price $K$ 
at time $T$. If the owner can exercise the option any time up to $T$, we call 
this an American option. For the purposes of this research, I will only be 
dealing with vanilla European options.\\ 
It is important to define the payoffs for both options: 
\begin{equation}
C_T = max(0,S_T-K)
\end{equation}
\begin{equation}
P_T = max(0,K-S_T)
\end{equation}



\subsection{Market Data}
In this research I will be focussing on hedging a portfolio consisting of a single 
asset, hence requiring a simulation of a single underlying. 
\subsubsection{Euro Stoxx 50}
The Euro Stoxx 50 Index (SX5E) and relevant derivatives. This is a stock index of 50 stocks in the Eurozone. 
This index captures around 60\% of the free-float market capitalisation of the 
Euro Stoxx Total Market Index which covers about 95\% of the free-float market 
in the Eurozone\autocite{a2021_euro}. Rationale behind choosing this index is the availability of data,
options traded with SX5E as the underlying and the liquidity of the index.
\\
Derivatives that are held in the portfolio to be hedged will include those that 
have SX5E as the underlying, examples are weekly, monthly, and quarterly
expiration options. These are European-style so can only be exercised upon 
maturity. Data can be found on Bloomberg\autocite{bloomberg_2023_bloomberg} and Refinitiv
\autocite{lseg}.
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.35]{sx5e.png}
    \caption{Euro Stoxx 50 price chart}
\end{figure}

\subsubsection{Brent Crude Oil}
Commodities often have complex dynamics, driven by geopolitical events and natural 
supply and demand. As well as this, oil in particular lends itself to increased
volatility often responding to changes by the Federal Reserve and OPEC(Organisation 
of petroleum-exporting countries). These factors create characteristics such as 
heavy tails and jumps, often not being represented well by traditional models. 
I will be using Brent Oil as a benchmark asset to compare the expressibility of 
quantum derived models vs existing classical ones. 
\clearpage

\section{Merton-Jump Diffusion Model}
My model of choice for comparison is the Merton-Jump Diffusion model, this an 
elementary model that goes beyond Black-Scholes by trying to capture the negative
skewness and excess kurtosis of log price returns. This is done through the 
addition of a compound Poisson jump process. This aims to represent the jumps 
we observe in the market in a more realistic fashion rather than assuming constant 
volatility assumption made by Black-Scholes. As a simplification, I will be referring 
to Black-Scholes by BS and Merton-Jump Diffusion with MJD.
\subsection{Black-Scholes}
Let's start with the Black-Scholes model, an elementary model first proposed in 
(give year here) to price European vanilla options. \\
Consider a call options on a non-dividend paying stock with expiry T and strike K. 
We will assume that the asset price will obey geometric brownian motion hence 
giving: 
\begin{equation}
dS_t = \mu {S_t} dt + \sigma S_t dW_t
\end{equation}
where $W_t$ is a standard Brownian motion. We assume interest rates to be constant,
meaning a unit of a given currency at time t will be worth $e^{rt}$ at time t.\\
We can now consider the value of our call option, $C$, at time t, by way of It$\^{o}$s 
lemma
we can say that 
\begin{equation}
dC = (\muS)
\end{equation}
\subsection{Model}
A standard derivation of the model will allow us to explore its assumptions and 
limitations. This model consists of two components, jump and diffusion. The 
diffusion will be modelled using a Weiner process and log-normal jumps driven 
by a Poisson process. This gives us the following
SDE. 
\begin{equation}
  \italic{d}S_t = (\alpha - \lambda k)S_tdt + \sigma S_t dW_t + (y_t-1)S_tdN_t
\end{equation}
where $W_t \text{ and } N_t$ are Weiner and Poisson processes respectively. 
$\lambda$ represents the intensity of the jumps, $\alpha$ is the drift rate
(expected return), and $k$ is the expected jump size. 
Solving the equation gives us an exponential L\'{e}vy model described by 
\begin{equation}
  S_t = S_0e^{\mathcal{L}_t}
\end{equation}
where $S_t$ is the stock price at time t, $S_0$ is the initial stock price. 
We can also define $\mathcal{L}_t$ to be 
\begin{equation}
  \mathcal{L}_t = (\alpha - \frac{\sigma^2}{2}-\lambda k)t + \sigma W_t + 
  \sum^{N_t}_{i=1}Y_i
\end{equation}
\subsection{Assumptions \& Limitations}
Through inspection of the equations, we can observe the following assumptions:
\begin{enumerate}
\item The asset price experiences continuous, random fluctuations over time,
  governed by Brownian motion (GBM)
\item The asset price experiences sudden, discontinuous jumps modelled by a 
  Poisson process, occurring at a constant rate $\lambda$
\item Jumps sizes are assumed to be log-normal $ln(y_t) \sim \mathcal{N}(\mu,\sigma^2)$
\end{enumerate}
Starting with the first assumption, we can see that assuming GBM may produce
unrealistic behaviour, most important being a lack of excess kurtosis.
Markets 
often exhibit fat tails, especially within commodities. One such event may 
be the release of news from OPEC+, the organisation of petroleum-exporting 
countries. A restriction in oil production may cause the price of oil to jump 
rapidly. In recent times, wars and conflict has also become 
ever present, causing large movements in asset prices; therefore it is not 
unrealistic to
expect extreme price movements to be more frequent than can be modelled 
by a Gaussian.
\newp
The MJD requires calibration of parameters before use, typically done using historical 
data or implied volatility surfaces. Once calibrated these become assumptions of 
the data and so do not change even if the market observations move away from it. 
This would lead us to expect higher overfitting to the historical data, possibly 
failing in unseen conditions such as the market's reaction to COVID. 
\newp 
We also may expect poor volatility clustering with the MJD; constant volatility 
is not experienced by the market, instead periods of high volatility followed 
by periods of low volatility is observed. Though this paper won't be focussing 
on this phenomenon, it is important to consider.
\subsection{Calibration}
In this research, I have chosen to use maximum likelihood estimation to estimate 
the parameters for the MJD model. In the analytical solution we require five 
parameters: $\alpha$, $\sigma$, $\mu_j$, $\delta$, and $\lambda$. These are the 
expected return, volatility of the given asset, expectation of the jump size, 
standard deviation of the jump size and lastly the jump intensity. We can then 
use MLE on the probability density of log returns $S_t = ln(\frac{S_t}{S_0})$ 
\begin{equation}
P(S_t) = \sum^\infty_{i=0} \frac{e^{-\lambda t}(\lambda t)^i}{i!}N(S_t;(\alpha - 
\frac{\sigma^2}{2}-\lambda k)t+i\mu_j,\sigma^2t+i\delta^2)
\end{equation}
The likelihood function hence becomes 
\begin{equation}
  L(\theta;S) = \prod^T_{t=1}P(S_t)
\end{equation}
We can minimise the negative log-likelihood to obtain 
\begin{equation}
  -\ln L(\theta;S) = -\sum^T_{t=1}\ln P(S_t)
\end{equation}
Another popular option to calibrate the MJD model is by considering the implied 
volatility surface of existing options. This technique can lead to a 
calibration but suffers with issues surrounding the sensitivity of the tails of 
the asset prices. It is also well documented that given a function that measures 
the calibration error, we can observe a largely flat landscape surrounding the 
optimal solution, 
implying obtaining accurate parameters can become very computationally 
expensive, often requiring hundreds of iterations \autocite{jump05}. These difficulties
can translate into a poor hedge, leaving a buyer overexposed to market fluctuations. 

\clearpage


\section{Quantum Computing}
This section aims to serve as a brief introduction to quantum computing, this may prove 
to be pedestrian to the experienced but is included to maintain the accessibility 
of this research report.
\subsection{Quantum Systems}
Unlike classical computing, quantum computing acts in a non-deterministic manner,
the computer remains in multiple states with given probabilities rather than a
fixed resultant state as expected from classical computers. Formally we can define 
a qubit to be a quantum system where the states of 0 and 1 are represented by a pair 
of normalised and mutually orthogonal quantum states $|0\rangle$ and $|1\rangle$. 
Intuitively however, let's start with a two-state machine; 
we can describe such system to be in the state 
$|0\rangle$ with some amplitude $\alpha$ and in $|1\rangle$ with amplitude $\beta$. 
This can be represented  as
\begin{equation}\label{eq51}
|\psi\rangle = \alpha|0\rangle + \beta|1\rangle
\end{equation}
for 
some $\alpha$ and $\beta$ such that $|\alpha|^2+|\beta|^2 = 1$; 
we can refer to this as a superposition. 
If measured in the standard basis, we would expect the outcome to be $|k\rangle$ 
with a certain probability, this outcome resulting in the output state of the 
measurement gate to also be $|k\rangle$. This would mean our state $|\psi\rangle$
is irreversibly lost; we refer to this as a collapse of state. Each qubit can 
be thought of as a vector, $\bold{v}$, on a Bloch's sphere which can be represented in two 
basis: $\theta$ and $\psi$. $\theta$ is the angle between $\bold{v}$ and the 
z-axis. $\psi$ becomes the angle between $\bold{v}$ and the x-axis. Considering 
a more general parameterisation of 
\ref{eq51} gives us 
\begin{equation}
  |\psi\rangle = \cos (\frac{\theta}{2})e^{i\psi_0}|0\rangle + \sin(\frac{\theta}
  {2})e^{i\psi_{1}}|1\rangle
\end{equation}
TALK MORE HERE
\newp
When extending to a n-qubit system
\newp
Before forming quantum circuits, we must first understand how quantum gates operate. 
Quantum gates can be thought of as fixed unitary operations on selected qubits, 
often represented with a $U$. There are many gates but the ones we are concerned 
with for this project are the $T$, $Rx$, $Ry$, $Rz$ and $CNOT$ gates. Given an 
angle $\theta$ we can form the universal gates: 
$$
R_x(\theta)& = \coloneqq 
\begin{bmatrix}
\cos(\theta/2) & -i\sin(\theta/2) \\
-i\sin(\theta/2) & \cos(\theta/2)
\end{bmatrix}
$$
$$
R_y(\theta) = \coloneqq 
\begin{bmatrix}
\cos(\theta/2) & -\sin(\theta/2) \\
\sin(\theta/2) & \cos(\theta/2)
\end{bmatrix}
$$
$$
R_z(\theta) = \coloneqq 
\begin{bmatrix}
e^{-i\theta/2} & 0 \\
0 & e^{i\theta/2}
\end{bmatrix}
$$
$$
T = \coloneqq 
\begin{bmatrix}
1 & 0 \\
0 & e^{-i\pi/4}
\end{bmatrix}
$$
$$
CNOT =
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0
\end{pmatrix}
$$
Universal gates, Clifford Gates and non-Clifford gates 
\subsection{Born Rule}
An essential part of quantum computing involves the existence of the Born
Rule. Born's measurement rule states that:
\begin{equation}
p(x) = |\langle x|\psi(\theta)\rangle|^2
\end{equation} where 
\begin{equation}
|\psi(\theta)\rangle = U(\theta)|0\rangle^{\otimes n}
\end{equation}
The state $|\psi(\theta)\rangle$ is generated by evolving state $|0\rangle$
according to a Hamiltonian $H$ that is constructed from gates. Once combined, the 
gates form a parameterised quantum circuit which is parameterised by using the 
variables governing each gate, $\theta$. By tuning the values of $\theta_i$ one 
can allow for an evolution to any state that will serve as a solution to a given 
problem. \\ 
By taking the distribution associated to the state, $|\psi(\theta)\rangle$ we can 
treat the PQC as a generative model, upon measurement will 
generate samples of a target distribution $\chi$. This model is parameterised 
by $\theta$, which defines a quantum circuit $U(\theta)$ made up of a set of quantum 
gates.
By measuring the circuit, we can obtain samples. Producing samples that emulate 
the target distribution involves minimising the parameters of the circuit $U(\theta)$, 
a process once convergence is reached, will generate accurate samples 
\cite{liu_differentiable_2018}.

\subsection{State Preparation}
We require state preparation to transfer the classical data onto the 
Hilbert space. This involves a function $\phi$ that maps the input vector to 
an output label. There are many encoding schemes, each of which aim to offer 
high information density and low error rates; main methods include: basis, amplitude,
angle encoding, and QRAM. 
\\
Without the use of ancillary qubits, we can expect an exponential circuit depth
to prepare an arbitrary quantum state. Using them we can reduce the depth to be 
sub-exponential scaling, with recent advancements reaching $\Theta(n)$ given $O(n^2)$
ancillary qubits
\cite{shaib_efficient_2023,zhang_quantum_2022}.



\subsection{Parameterised Quantum Circuits}
Parameterised quantum circuits.

\clearpage
\section{Quantum Circuit Born Machine}
Given a dataset $D = \{x_1, x_2.. x_n\}$ consisting of n samples and obeys a 
given distribution $\chi_d$, we would like the QCBM to learn the distribution 
and generate synthetic data points that are of the distribution $\chi_s$ such that
$\chi_s$ approximates $\chi_d$.
The QCBM is a subclass of parameterised quantum circuits, 
here the quantum circuit contains parameters which are updated during a training 
process. The QCBM takes the product state $\ket{0}$ as an input, and through an 
evolution, transforms into a final state $\ket{\phi_0}$ by a sequence of unitary 
gates. This can then be measured to obtain a sample of bits 
$x \sim p_\theta (x_s)=|\bra{x}\ket{\phi_\theta}|^2$ . By training the model we 
are aiming to let $p_\theta$ approach $\chi_d$. 
\\
The ansatz for this quantum circuit consists of 7 layers
of 1-qubit gates with entangling layers in between them. These are 
entangled using the CNOT gates as found in the appendix. The number of wires 
needed depends on the precision required for the generated data. The estimated
precision is 12-bit, so the samples are able to take $2^{12}$ different values in 
the range of $ (v_{min} - \epsilon, v_{max} + \epsilon )$, where $\epsilon > 0 $
allows data to be generated that lie outside the range $(v_{min},v_{max})$ of the 
original data.
\\
The QCBM takes a $n \times m$ matrix of parameters in the range $(-\pi, \pi)$ as 
input, in the form of a dictionary. Each angle takes one of $2^k$ discrete values, 
where $k$ is a model parameter. The resulting space therefore spans to: 
$(2^m)^{n\cdot m}$.


\subsection{Barren Plateau}
A point of concern when searching for the optimal set of $\theta s$ is the large 
search space, here we may observe issues such as barren plateau(BP). BP 
insists that the gradient of the parameters of a given PQC will vanish exponentially 
w.r.t the search space.
\\ 
Introduce proof of $\frac{\partial C}{\partial \theta} \rightarrow 0 $


\subsection{Architectures}
The design of the ansatz can significantly affect the ability to learn and represent 
the target distribution. The number of gates, depth, and entanglement structure
all affect the expressibility and trainability of the circuit. Circuits with 
higher entanglement and parameterised gates are theoretically able to represent 
any distribution, though it comes at the cost of noise and barren plateaus. 
Choosing a simpler ansatz may converge quicker but results in a weaker approximation,
oversimplifying the solution. 
\\
The architecture will also affect the optimisation landscape; random parameterised 
circuits with deep, unstructured layers are more prone to barren plateaus

Due to this we strive for a balance, one that is 
able to learn the complexities of the market without compromising on finding the 
optimal parameters. 
\subsubsection{Brick}
\subsubsection{Pyramid}
\subsubsection{Butterfly}




\subsection{ZX Calculus}
ZX Calculus as a method to optimise/minimise t gate count. introduce what a 
non clifford gate is.


\newpage
\section{Results}
Putting the theory into practice offered insights into the strengths and weaknesses 
of the model. This section aims to provide quantitative comparisons between the 
classical and quantum methods. After careful consideration and analysis of parameters 
for the QCBM, the model used for comparisons against the MJD model is a 13 qubit,
7 layer model. It uses the brick architecture and has been trained for 500 epochs 
with an Adam optimiser. 
\subsection{Path Generation}
An important part of risk analysis involves path generation, simulating an equity 
path for the next $n$ days. This provides a range of final values, aiming to 
simulate price paths accurately in the process. The metrics involved in the analysis 
involves comparing: the skewness, excess kurtosis, and standard deviation. 
For a return $r_i$ and mean return $\bar{r}$ 
\\Standard deviation:
$$
\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (r_i - \bar{r})^2}
$$
Skewness:
$$
\gamma_1 = \frac{\sum_{i=1}^{N} (r_i - \bar{r})^3}{(N-1) \times \sigma^3}
$$

Excess kurtosis:
$$
\gamma_2 = \frac{1}{N}\frac{\sum_{i=1}^{N} (r_i - \bar{r})^4}{\sigma^4} - 3
$$
These 
were chosen to highlight the accuracy of the models as well as the ability to represent 
subtleties in the data such as the asymmetric nature and fat tails that are often 
present in market data. Both models were first calibrated on Brent Crude Oil stock 
prices. The test period combines the training and unseen data to see how well the 
model is able to perform on out-of-sample prices. The period of data includes the 
recent reaction to Trump's tariffs, April 2025. This was included purposely to observe 
the model's resilience to tough market activity and heightened volatility.
\begin{table}[h!]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Drift ($\mu$) & 0.37956244 \\
Volatility ($\sigma$) & 0.40736202 \\
Jump Intensity ($\lambda$) & 530.63931692 \\
Jump Mean ($\mu_J$) & -0.00374471 \\
Jump Variance ($\sigma_J^2$) & 0.00065703 \\
\hline
\end{tabular}
\caption{Calibrated Parameters of the MJD Model for Brent Crude Oil}
\label{tab:mjd_params}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lccccc}
\hline
\textbf{} & \textbf{Original Data} & \textbf{Test Data} & \textbf{MJD Data} & \textbf{QCBM Data} \\
\hline 
Standard Deviation & 0.0233 &0.0225 & 0.02647 & 0.0268   \\
Skewness            & 0.6550 &0.6709 & -0.2579 & 0.6215 \\
Excess Kurtosis     & 8.5986 &8.8553 & 7.4809 & 8.7375  \\
\hline
\end{tabular}
\caption{Comparison of Brent data with MJD and QCBM Data}
\label{tab:brentdata}
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{lcccc}
\hline
\textbf{} & \textbf{Original Data} & \textbf{Test Data} & \textbf{MJD Data} & \textbf{QCBM Data} \\
\hline 
Standard Deviation  & 0.0120  & 0.0270  & 0.0126  & 0.0124 \\
Skewness            & 0.8602  & 1.1640  & -0.2015 & 0.7451 \\
Excess Kurtosis     & 11.5700 & 5.6518  & 11.4330 & 11.3780 \\
\hline
\end{tabular}
\caption{Comparison of Eurexx Data with MJD and QCBM Data}
\label{tab:eurexxdata}
\end{table}



As shown in table \ref{tab:brentdata}, the results highlighted that on training data 
for Brent Crude Oil, the 
MJD model was able to capture the standard deviation better, however the skewness 
and excess kurtosis was represented weakly. This is where the QCBM was able to 
demonstrate superiority, 
capturing the skewness and excess kurtosis with greater accuracy.
\newp 
Table \ref{tab:eurexxdata} supports most of the arguments made, though has shown 
slight superiority in representing the fat tails during the training period. 
As the Eurexx data contains less extreme jumps, we can see the MJD model perform 
better. That being said, we can still observe the skewness being learnt poorly
by the classical model. The test data chosen is a continuation of the training data,
both models showing clear difficulties in generalisation. This can be excused however 
as the metrics differ hugely from the training data. This does raise a point of 
concern whether these methods employed of market behaviour prediction are 
appropriate. For this reason, it is more appropriate to treat the models as a 
synthetic market generator rather than for the purposes of prediction. The differences 
in the MJD performance should be noted, highlighting how classical methods 
tend to struggle in tougher market conditions; this is will be further investigated 
in the following sections.
\begin{figure}[h!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{realpricepath.png}
        \caption{True price path}
        \label{fig:realpricepath}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{mjdpricepath.png}
        \caption{Price trajectories for MJD model}
        \label{fig:mjdpricepath}
    \end{minipage}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{compdists.png}
        \caption{Comparison of distributions}
        \label{fig:compdists}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{pricepath2.png}
        \caption{Price trajectories for QCBM model}
        \label{fig:pricepath}
    \end{minipage}
\end{figure}

A comparison of the log return distributions indicates the lack of tail representation 
with the MJD model. It is important to note that different parameters for the QCBM 
gave significantly different results. Adding only 1 more qubit led to an 
over-estimation in kurtosis as well as poor representation of the standard 
deviation. From figure \ref{fig:pricepath} we can see behaviour that would on 
first glance mimic the real market. Jumps in the market look realistic, and 
trajectories take believable paths.

\begin{figure}[h!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{logreturns.png}
        \caption{Log returns}
        \label{fig:compdists}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{cumsum.png}
        \caption{Cumulative sum of returns}
        \label{fig:pricepath}
    \end{minipage}
\end{figure}


\begin{figure}[h!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{weeklyacfog.png}
        \caption{Weekly ACF for training data}
        \label{fig:compdists}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{weeklyacfqcbm.png}
        \caption{Weekly ACF for QCBM data}
        \label{fig:pricepath}
    \end{minipage}
\end{figure}
\subsection{Volatility Analysis}
It is also important to analyse if the local behaviour of the equity paths is well 
captured. We can do this by analysing the volatility; here the shortfalls of the 
QCBM become clear. Though the global volatility distribution appears to well 
represented, using GARCH models shows a disparity in observed volatility clustering 
compared to the quantum generated volatility. This appears to be a fundamental 
flaw in the model. The assumption that we can represent a path using [give 
equation for propagating path] leads to poor volatility clustering as we have 
treated for each time $0\leq t_0 \leq ...\leq t_n$ our random variable 
$S_{t_r}-S_{t_{r-1}}$ are independent.
\newp 
First looking at rolling volatility, we observe acceptable performance by the 
MJD model, displaying realistic market volatility; the QCBM, however, displayed 
a worse performance, often underestimating volatility peaks, and overestimating 
noise as shown in figure \ref{fig:rollingvol}.
This heightened noise is also reflected 
in figure \ref{fig:weeklyvol} where we see on a weekly basis, the QCBM 
constantly overestimated the volatility. We can observe the difference in learnt 
volatility by comparing distributions. Figure \ref{fig:voldist2} shows us how the 
volatility is skewed with a larger mean and lack of kurtosis. 
\begin{figure}[h!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{rollingvol.png}
        \caption{Comparison of rolling volatility (20-day window)}
        \label{fig:rollingvol}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{weeklyvol.png}
        \caption{Comparison of weekly volatility}
        \label{fig:weeklyvol}
    \end{minipage}
\end{figure}
\begin{figure}[h!]
  \centering 
  \includegraphics[scale=0.35]{voldist2.png}
  \caption{Comparison of volatility distributions}
  \label{fig:voldist2}
\end{figure}
This story worsens as we look at using volatility models as a comparison method. 
\newp
GARCH (Generalised Autoregressive Conditional Heteroskedasticity) models were 
first introduced in 1982 by Robert Eagle [add reference here] to model volatility 
as a non-constant quantity in financial models. The GARCH(1,1) can be defined as 
so:\\ 
Let $r_t$ be the asset return at time $t$. This can be decomposed as 
\begin{equation}
r_t = \mu + \epsilon_t
\end{equation}
where $\mu$ is the mean return and $\epsilon_t = \sigma_t z_t$ where $z \sim N(0,1)$. 
GARCH models the conditional variance of a given time series process, asset returns in 
our world, as a function of past squared shocks ($\epsilon^2_{t-1}$) and past 
conditional variance ($\sigma^2_{t-1}$). Using this, our model equation becomes
\begin{equation}
  \sigma^2_t = \omega + \alpha\epsilon^2_{t-1} + \beta\sigma^2_{t-1}
\end{equation}
where $\omega > 0$ is the average volatility, $\alpha \geq 0$ is the sensitivity 
to recent shocks, and $\beta \geq0$ is the persistence of volatility, the tendency
of volatility to be high for periods of time and then low of periods of time. 
\newp 
A further model that improved on GARCH is the Exponential GARCH, another model used 
for comparison in my findings. This model focussed on asymmetric volatility 
effects, removing parameter restrictions, and having a logarithmic formulation. 
We can define an EGARCH(1,1) as follows 
\begin{equation}
  \ln(\sigma^2_t) = \omega +\beta\ln(\sigma^2_{t-1})+\gamma\frac{\epsilon_{t-1}}
  {\sigma_{t-1}} + \alpha\Biggl(\frac{|\epsilon_{t-1}|}{\sigma_{t-1}}-\sqrt{\frac{2}{\pi}}\Biggr)
\end{equation}
where the extra term $\gamma$ accounts for the leverage effect; the 
negative correlation between asset returns and volatility change often 
observed in markets.
\newp 
\begin{figure}[h!]
  \centering 
  \includegraphics[scale=0.4]{egarch2.png}
  \caption{EGARCH model fit to models}
  \label{fig:egarch2}
\end{figure}
The implementation and parameter estimation for $\omega$,$\alpha$, $\beta$ and 
$\gamma$ were all handled by 
the python package 'arch' [insert this reference https://pypi.org/project/arch/]. 
Calibrating a EGARCH(1,1) to the different model returns gave the following graph. 
In figure \ref{fig:egarch2} we can observe the MJD model displaying more accurate 
clustering compared to the QCBM which remains conservative. This may suggest that 
the MJD model is more suitable for predicting volatility over a short period of 
time. As an improvement to the QCBM, it may be of use to create a hybrid model, 
one that combines the QCBM and a volatility model of choice. 

\newpage
\subsection{VaR \& CVaR}
To determine the usefulness of the generator for hedging purposes, we must explore 
how both generators perform in the extreme percentiles. These are the scenarios 
that we call Black-Swan events; an event that is unexpected, infrequent but has 
huge financial implications. Not being hedged against such changes can leave an 
investor in unwanted positions, exposed to large changes and at risk of losing 
a lot of money. To quantify expected loss a market may give, we employ techniques 
such as VaR(Value at Risk) and CVaR(Conditional Value at Risk). 
\newp 
VaR is a measure that focuses on the loss at a given percentile under normal 
market conditions. It provides a threshold such that the probability of a loss 
exceeding a given value is a chosen percentile i.e 1\% or 5\%. An intuitive 
way to think about it is 1\% VaR of -0.05 means there is a 1\% chance of 
losing at least 5\% of the portfolio value. Mathematically we can define VaR:\\ 
Let $X$ be a random variable with cdf $F_{X}(z)=P\{X\leq z\}$, this is our loss function. 
The VaR of $X$ at a confidence level $\alpha$ is given by
\begin{equation}
  VaR_\alpha(X) = \min\{z|F_X(z)\geq \alpha\}
\end{equation}
VaR, however, has a few crucial limitations. VaR does not say anything about the 
size of losses made after the given $\alpha$ value. This makes it very hard to 
quantify the loss an investor may be exposed to. VaR also assumes liquidity in 
the market at any position when in reality during periods of market stress, the 
ability to buy or sell at any price can be very difficult. It should also be 
known that VaR has a non-subadditivity constraint, meaning VaR for different portfolios 
cannot be added together. For these reasons we opt to use Conditional Value at 
Risk, a more sophisticated tool designed to overcome the given limitations. 
\newp 
CVaR focusses on measuring the risk of extreme losses. Instead of considering of 
considering the probability of losing a given amount, we instead focus on how 
much is lost in the given quantile. This quantifies our tail risk, making it 
appropriate for evaluating the heavy tails of our market generators. Intuitively 
a 1\% CVaR of -0.05 means that, in the worst 1\% of cases, the average loss is 5\%. 
We can define CVaR at a level $\alpha$ as 
\begin{equation}
  CVaR_{\alpha}(X) = \mathbb{E}[X | X \geq VaR_{\alpha}(X)]
\end{equation}
This provides us with a sturdier framework to evaluate performances although we 
must still be careful of its assumptions and limitations. We must assume that the 
distribution of the loss is measured accurately and market conditions are also 
represented faithfully within the given time window. Once we accept these to be 
true, we are provided with a method to compare strategies and generators, as well 
as conforming to financial regulations such as Basel III.
\newp 
The first test was on Eurexx data from the COVID period. This was excluded from the training 
dataset to be used as an extreme market situation. Market generators need to be 
resilient and able to account such market shocks, these are the scenarios where 
hedging positions stop investors from losing large amounts of money.
\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\hline
\textbf{} & \textbf{VaR (1\%)} & \textbf{CVaR (1\%)} \\
\hline
Original Data     & -0.0171 & -0.0174 \\
QCBM Simulated    & -0.0291 & -0.0308 \\
MJDM Simulated    & -0.0360 & -0.0377 \\
\hline
\end{tabular}
\caption{VaR \& CVaR at the 1\% Quantile for COVID data}
\label{tab:cvar_1}
\end{table}
\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\hline
\textbf{} & \textbf{VaR (5\%)} & \textbf{CVaR (5\%)} \\
\hline
Original Data     & -0.0159 & -0.0167 \\
QCBM Simulated    & -0.0223 & -0.0270 \\
MJDM Simulated    & -0.0283 & -0.0338 \\
\hline
\end{tabular}
\caption{VaR \& CVaR at the 5\% Quantile for COVID data}
\label{tab:cvar_5}
\end{table}
At the 1\% quantile we can see both
the MJD and QCBM exhibit greater losses in the tails compared to the real data.
The QCBM has a VaR of 2.91\% and CVaR of 3.08\%, these are both significant 
overestimates of tail risk present in the training data. The MJD overestimates 
the tail risk even greater, expecting much larger than expected losses. We could 
explain this due to the jump component. These pessimistic scenarios, though 
stopping large losses, in reality may lead to over hedging, a situation where a 
given investor may be too protected to the market, limiting potential gains. These 
scenarios may be suboptimal for a deep hedging engine, where the optimal hedge 
is going to be larger than needed. 
\newp 
At the 5\% quantile we see a similar story fold out, both generators estimating 
the tail risk to be greater than observed. The QCBM once again provides a closer 
score to the data, indicating the model is more suitable for hedging purposes, 
though only marginally. What should be noted is the large gap between the VaR 
and CVaR, indicating the MJD model has accounted for significant market shocks,
indicating high sensitivity to extreme events.\\
\newp 
The next scenario is on the unseen continuation of the Brent Crude Oil dataset,
though not excluded purposely than just for out-of-sample testing, the market 
conditions observed have been very volatile. As mentioned earlier, the effect of 
Trump's tariffs were felt in all markets, particularly affecting assets denominated 
in the US dollar. Though as unfortunate as the scenario may be to investors, this 
has provided me with an excellent test for the two models. 
\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\hline
\textbf{} & \textbf{VaR (1\%)} & \textbf{CVaR (1\%)} \\
\hline
Original Data     & -0.0300 & -0.0342 \\
QCBM Simulated    & -0.0429 & -0.0524 \\
MJDM Simulated    & -0.0522 & -0.0631 \\
\hline
\end{tabular}
\caption{VaR \& CVaR at the 1\% Quantile for Brent Crude Data}
\label{tab:cvar_1_brent}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\hline
\textbf{} & \textbf{VaR (5\%)} & \textbf{CVaR (5\%)} \\
\hline
Original Data     & -0.0239 & -0.0274 \\
QCBM Simulated    & -0.0313 & -0.0391 \\
MJDM Simulated    & -0.0414 & -0.0508 \\
\hline
\end{tabular}
\caption{VaR \& CVaR at the 5\% Quantile for Brent Crude Data}
\label{tab:cvar_5_brent}
\end{table}
Though the data was more volatile, both models demonstrated that they 
are able to capture the tail risk. The training dataset had large amounts of 
volatility so we would expect good representation of tail risk. 
As seen with the COVID results, 
the models are overestimating the downside risk with scenarios showing 
greater loss than expected. The large gap between the VaR and CVaR values once 
again tells us that the underlying models predict tail events far higher than 
observed in the market. It should be said that the QCBM model is able to reflect 
the tail risk more accurately than the MJD, with value that are closer to the 
original data. One might make conclusions that this makes the QCBM a useful model 
but the poor performance to the original data raises concerns; we can however 
say that the model is more appropriate to use than the MJD in this scenario. 



\newpage
\subsection{Hedging}

\subsection{Barren Plateau}

\subsubsection{ZX Calculus}

\clearpage 

\section{Outlook and Conclusions}

[add in the conclusion, large hedges are suboptimal, and traders underhedge anyways]

\clearpage
\section{Appendix}
\subsection{QCBM Architectures}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.3, angle=270, width=\textwidth-209]{qcbm1.png}
    \caption{QCBM architecture}
\end{figure}
\clearpage


\printbibliography

\end{document}

